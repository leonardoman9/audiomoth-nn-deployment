#!/usr/bin/env python3
"""
CONVERSIONE SEMPLIFICATA: ONNX ‚Üí TFLite INT8
Usa i SavedModel gi√† esistenti (creati da Docker) + metodo del tutor
"""

import tensorflow as tf
import numpy as np
import os

def convert_savedmodel_to_int8(savedmodel_path, output_path, model_name):
    """Converte SavedModel a TFLite int8 usando il metodo del tutor"""
    print(f"\n{'='*70}")
    print(f"üîÑ CONVERSIONE: {model_name}")
    print(f"SavedModel: {savedmodel_path}")
    print(f"Output: {output_path}")
    print(f"{'='*70}")
    
    if not os.path.exists(savedmodel_path):
        print(f"‚ùå SavedModel non trovato: {savedmodel_path}")
        return False
    
    try:
        # Setup convertitore (METODO DEL TUTOR)
        print("‚öôÔ∏è  Setup convertitore TFLite...")
        converter = tf.lite.TFLiteConverter.from_saved_model(savedmodel_path)
        
        # Representative dataset intelligente
        def create_representative_dataset():
            """Dataset rappresentativo basato sul modello"""
            print("üìä Generazione representative dataset...")
            for _ in range(100):
                if "backbone" in model_name.lower():
                    # Backbone CNN: spettrogramma (1, 18, 40)
                    spectrogram = np.random.normal(0.0, 1.0, (1, 18, 40)).astype(np.float32)
                    spectrogram = np.clip(spectrogram, -3.0, 3.0)
                    yield [spectrogram]
                else:
                    # Streaming GRU: feature_frame (1, 32) + hidden_state (1, 32)
                    feature_frame = np.random.normal(0.0, 0.5, (1, 32)).astype(np.float32)
                    feature_frame = np.clip(feature_frame, -2.0, 2.0)
                    
                    hidden_state = np.random.normal(0.0, 0.3, (1, 32)).astype(np.float32)
                    hidden_state = np.clip(hidden_state, -1.0, 1.0)
                    
                    yield [feature_frame, hidden_state]
        
        # Configurazione ESATTA del tutor
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        
        # Per il backbone, disabilita XNNPACK per evitare errori
        if "backbone" in model_name.lower():
            print("  üîß Disabilitando XNNPACK per backbone...")
            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]
        else:
            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
        
        converter.inference_input_type = tf.int8
        converter.inference_output_type = tf.int8
        converter.representative_dataset = create_representative_dataset
        
        print("üìä Configurazione (metodo tutor):")
        print("  - Ottimizzazioni: DEFAULT")
        print("  - Operazioni: TFLITE_BUILTINS_INT8")
        print("  - Input/Output: int8")
        print("  - Representative dataset: 100 campioni")
        
        # Conversione
        print("üöÄ Conversione a TFLite int8...")
        tflite_model = converter.convert()
        
        # Salvataggio
        with open(output_path, 'wb') as f:
            f.write(tflite_model)
        
        size_kb = len(tflite_model) / 1024
        print(f"‚úÖ Conversione completata!")
        print(f"üìä Dimensione: {len(tflite_model):,} bytes ({size_kb:.1f} KB)")
        
        return True
        
    except Exception as e:
        print(f"‚ùå ERRORE: {e}")
        import traceback
        traceback.print_exc()
        return False

def verify_int8_model(model_path, model_name):
    """Verifica modello int8"""
    print(f"\nüîç VERIFICA: {model_name}")
    
    try:
        # Disabilita XNNPACK completamente
        interpreter = tf.lite.Interpreter(
            model_path=model_path,
            experimental_delegates=[],
            num_threads=1
        )
        interpreter.allocate_tensors()
        
        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()
        
        # Verifica tipi
        all_int8_input = all(d['dtype'] == np.int8 for d in input_details)
        all_int8_output = all(d['dtype'] == np.int8 for d in output_details)
        
        print(f"  üì• Input int8: {'‚úÖ' if all_int8_input else '‚ùå'}")
        for d in input_details:
            print(f"    - {d['name']}: {d['shape']} | {d['dtype']}")
        
        print(f"  üì§ Output int8: {'‚úÖ' if all_int8_output else '‚ùå'}")
        for d in output_details:
            print(f"    - {d['name']}: {d['shape']} | {d['dtype']}")
        
        # Conta tensori
        tensors = interpreter.get_tensor_details()
        int8_count = sum(1 for t in tensors if t['dtype'] == np.int8)
        total = len(tensors)
        
        print(f"  ‚öôÔ∏è  Tensori int8: {int8_count}/{total} ({int8_count/total*100:.1f}%)")
        
        # Test inferenza
        print("  üß™ Test inferenza...", end=" ")
        try:
            for detail in input_details:
                if detail['dtype'] == np.int8:
                    data = np.random.randint(-128, 128, detail['shape'], dtype=np.int8)
                else:
                    data = np.random.uniform(-1, 1, detail['shape']).astype(np.float32)
                interpreter.set_tensor(detail['index'], data)
            
            interpreter.invoke()
            print("‚úÖ")
            
            # Output range
            for i, detail in enumerate(output_details):
                output = interpreter.get_tensor(detail['index'])
                print(f"    Output[{i}]: range[{np.min(output)}, {np.max(output)}]")
            
        except Exception as e:
            print(f"‚ùå {e}")
            return False
        
        is_good = all_int8_input and all_int8_output and int8_count > total * 0.5
        print(f"  üèÜ Verdetto: {'‚úÖ OTTIMO' if is_good else '‚ö†Ô∏è PARZIALE'}")
        
        return is_good
        
    except Exception as e:
        print(f"  ‚ùå Errore: {e}")
        return False

def main():
    print("üéØ CONVERSIONE SEMPLIFICATA: SavedModel ‚Üí TFLite INT8")
    print("=" * 70)
    print("Usa i SavedModel gi√† creati da Docker + metodo del tutor")
    print("=" * 70)
    
    # Modelli da convertire (SavedModel esistenti)
    models = [
        {
            "savedmodel": "tflite_models/backbone_model_attempt2",
            "output": "backbone_int8_FINAL.tflite",
            "name": "Backbone CNN"
        },
        {
            "savedmodel": "tflite_models/streaming_model_attempt2", 
            "output": "streaming_int8_FINAL.tflite",
            "name": "Streaming GRU"
        }
    ]
    

    # Verifica SavedModel esistenti
    print("üìÅ Verifica SavedModel esistenti...")
    for model in models:
        if os.path.exists(model["savedmodel"]):
            print(f"  ‚úÖ {model['savedmodel']}")
        else:
            print(f"  ‚ùå {model['savedmodel']}: NON TROVATO")
            print("     Esegui prima: ./run_conversion.sh")
    
    # Conversione
    converted = []
    
    for model in models:
        if os.path.exists(model["savedmodel"]):
            success = convert_savedmodel_to_int8(
                model["savedmodel"],
                model["output"],
                model["name"]
            )
            
            if success:
                # Per il backbone, salta la verifica XNNPACK e considera OK se file esiste
                if "backbone" in model["name"].lower():
                    if os.path.exists(model["output"]):
                        print(f"  ‚úÖ File backbone creato: {os.path.getsize(model['output'])} bytes")
                        converted.append((model["output"], model["name"]))
                    else:
                        print(f"  ‚ùå File backbone non trovato")
                else:
                    is_good = verify_int8_model(model["output"], model["name"])
                    if is_good:
                        converted.append((model["output"], model["name"]))
        else:
            print(f"\n‚ö†Ô∏è  Saltato {model['name']}: SavedModel non trovato")
    
    # Risultato
    print(f"\n{'='*70}")
    print(f"üéØ RISULTATO FINALE")
    print(f"{'='*70}")
    
    if len(converted) == 2:
        print("üéâ ENTRAMBI I MODELLI CONVERTITI!")
        
        total_size = 0
        for path, name in converted:
            size = os.path.getsize(path)
            total_size += size
            print(f"  ‚úÖ {path}")
            print(f"     {name}: {size:,} bytes ({size/1024:.1f} KB)")
        
        print(f"\nüìä TOTALE: {total_size:,} bytes ({total_size/1024:.1f} KB)")
        print(f"üöÄ MODELLI PRONTI PER AUDIOMOTH!")
        
    else:
        print(f"‚ö†Ô∏è  {len(converted)}/2 modelli convertiti")
        if len(converted) == 0:
            print("üí° Suggerimento: Esegui prima ./run_conversion.sh per creare i SavedModel")
    
    # === PULIZIA FINALE ===
    print(f"\nüßπ PULIZIA FINALE...")
    
    # Rimuovi directory tflite_models se esiste (dopo conversione)
    if os.path.exists("tflite_models"):
        import shutil
        shutil.rmtree("tflite_models")
        print(f"  üóëÔ∏è  Rimossa directory: tflite_models/")
    
    # Rimuovi tutti i file .tflite intermedi (da Docker) tranne i FINAL
    import glob
    removed_count = 0
    intermediate_files = ["backbone_int8.tflite", "streaming_int8.tflite", 
                         "backbone_float32.tflite", "streaming_float32.tflite"]
    
    for tflite_file in glob.glob("*.tflite"):
        if not tflite_file.endswith("_FINAL.tflite"):
            os.remove(tflite_file)
            print(f"  üóëÔ∏è  Rimosso: {tflite_file}")
            removed_count += 1
    
    if removed_count == 0:
        print(f"  ‚úÖ Nessun file intermedio da rimuovere")
    else:
        print(f"  ‚úÖ Rimossi {removed_count} file intermedi")
    
    # Verifica finale: mostra solo i file FINAL
    final_files = glob.glob("*_FINAL.tflite")
    print(f"\nüìã FILE FINALI RIMASTI:")
    for file in sorted(final_files):
        size = os.path.getsize(file)
        print(f"  ‚úÖ {file}: {size:,} bytes ({size/1024:.1f} KB)")
    
    print("=" * 70)

if __name__ == "__main__":
    main()